{"cells": [{"cell_type": "code", "execution_count": 55, "id": "74519aa9-1575-470e-af96-83e69b98ef54", "metadata": {}, "outputs": [], "source": "import json\nimport sys\nfrom datetime import datetime\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import (udf, col, expr)\n\nfrom google.cloud import storage\nfrom time import time\n\nspark = SparkSession.builder.appName(\"censo\").getOrCreate()\nspark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"}, {"cell_type": "code", "execution_count": 56, "id": "85345474-e475-4829-b948-fe35aae41b4a", "metadata": {}, "outputs": [], "source": "def add_prefix_in_columns(df, prefix):\n    return df.select([col(column).alias(f\"{prefix}_{column}\") for column in df.columns])\n\ndef load_json(name, bucket):\n    bucket = storage.Client().get_bucket(bucket)\n    blob = bucket.blob(f'etl/censo_escolar/transformation/{name}.json')\n    maps = json.loads(blob.download_as_string())\n    return maps\n\n\ndef mapping(df, map_, column, type_return):\n    map_func = udf(lambda key: map_.get(str(key)), type_return())\n    df = df.withColumn(column, map_func(col(column)))\n    return df\n\ndef string_to_date(df, column, year):\n    if year > 2014:\n        pattern = '%d/%m/%Y'\n    else:\n        pattern = \"%d%b%Y:%H:%M:%S\"\n    map_func =  udf (lambda date: datetime.strptime(date, pattern) \n                     if type(date) == str \n                     else None, DateType())\n    df = df.withColumn(column, map_func(col(column)))\n    return df\n\ndef load_csv(file, bucket, year, region=None):\n    schema = load_json(f\"schemas/{file}_schema\", bucket)\n    try:\n        schema = StructType.fromJson(schema)\n    except:\n        schema = StructType.fromJson(json.loads(schema))\n        \n    if file == \"gestor\" and int(year) < 2019:\n        df = spark.createDataFrame(data=[],schema=schema)\n    else:\n        if file in [\"matricula\", \"docentes\"] :\n            file = f\"gs://{bucket}/landing_zone/censo-escolar/{year}/{file}_{region}.csv\"\n        else:\n            file = f\"gs://{bucket}/landing_zone/censo-escolar/{year}/{file}.csv\"\n\n        df = spark \\\n                .read \\\n                .options(header=True, delimiter=\"|\", encoding=\"utf8\") \\\n                .schema(schema=schema) \\\n                .csv(file)\n    return df\n\ndef transform_string_columns(df, file, bucket):\n    maps = load_json(\"maps\", bucket)\n    string_columns = [column for column in df.columns \n                      if column.startswith(\"TP\") or column.startswith(\"CO\")]\n\n    for column in string_columns:\n        if column in maps:\n            df = mapping(df, maps[column], column, StringType)\n            \n    return df\n\ndef transform_boolean_columns(df):\n    boolean_columns = [column for column in df.columns \n                  if column.startswith(\"IN\")]\n\n    mapping_bool = {\n        \"0\": False,\n        \"1\": True\n    }\n\n    for column in boolean_columns:\n        df = mapping(df, mapping_bool, column, BooleanType)\n        \n    return df\n        \ndef transform_integer_columns(df):\n    integer_columns = [column for column in df.columns \n                      if column.startswith(\"NU\") or column.startswith(\"QT\")]\n    for column in integer_columns:\n        df = df.withColumn(column, col(column).cast(IntegerType()))\n    \n    return df\n\ndef transform_date_columns(df, file):\n    if file == \"escolas\":\n        df = string_to_date(df, \"DT_ANO_LETIVO_INICIO\", YEAR)\n        df = string_to_date(df, \"DT_ANO_LETIVO_TERMINO\", YEAR)\n    \n    return df\n\ndef drop_columns(df, file):\n    drops = []\n    if file in [\"turmas\", \"matricula\", \"gestor\", \"docentes\"]:\n        drops.extend([\"NU_ANO_CENSO\",\n                     'TP_REGULAMENTACAO',\n                     'CO_UF',\n                     'IN_MANT_ESCOLA_PRIVADA_ONG',\n                     'NU_ANO_CENSO',\n                     'CO_MUNICIPIO',\n                     'IN_CONVENIADA_PP',\n                     'IN_ESPECIAL_EXCLUSIVA',\n                     'TP_CATEGORIA_ESCOLA_PRIVADA',\n                     'IN_MANT_ESCOLA_PRIVADA_OSCIP',\n                     'IN_MANT_ESCOLA_PRIV_ONG_OSCIP',\n                     'IN_MANT_ESCOLA_PRIVADA_S_FINS',\n                     'IN_MANT_ESCOLA_PRIVADA_SIST_S',\n                     'CO_DISTRITO',\n                     'IN_EDUCACAO_INDIGENA',\n                     'CO_MICRORREGIAO',\n                     'TP_DEPENDENCIA',\n                     'IN_EJA',\n                     'IN_REGULAR',\n                     'IN_PROFISSIONALIZANTE',\n                     'TP_LOCALIZACAO_DIFERENCIADA',\n                     'TP_CONVENIO_PODER_PUBLICO',\n                     'TP_LOCALIZACAO',\n                     'CO_REGIAO',\n                     'CO_MESORREGIAO',\n                     'IN_MANT_ESCOLA_PRIVADA_EMP',\n                     'IN_MANT_ESCOLA_PRIVADA_SIND',\n                    ]\n                )\n    if file in [\"matricula\", \"docentes\"]:\n        drops.extend([\"CO_ENTIDADE\"])\n        \n    if file == 'matricula':\n        drops.extend([ 'NU_DIAS_ATIVIDADE', \n                         'NU_DURACAO_TURMA'])\n        \n    if file != \"turmas\":\n        drops.extend([\"TP_MEDIACAO_DIDATICO_PEDAGO\", \n                      \"TP_TIPO_ATENDIMENTO_TURMA\",\n                      \"TP_TIPO_LOCAL_TURMA\"])\n    \n        \n    df = df.drop(*drops)\n\n    return df\n        \n\ndef transform(file, bucket, year, region=None):\n        df = load_csv(file, bucket, year, region)\n        df = drop_columns(df, file)\n        df = transform_string_columns(df, file, bucket)\n        df = transform_boolean_columns(df)\n        df = transform_integer_columns(df)\n        df = transform_date_columns(df, bucket)\n        return df"}, {"cell_type": "code", "execution_count": null, "id": "17322978-c7ac-4ed5-98ae-567c64a991cd", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "21/07/29 02:02:50 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1408.2 KiB\n21/07/29 02:03:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 3.8 MiB\n[Stage 163:============>  (13 + 3) / 16][Stage 166:>              (0 + 13) / 16]\r"}], "source": "regions =  [\"co\", \"nordeste\", \"norte\", \"sudeste\", \"sul\"]\npartitions = [\"E_NU_ANO_CENSO\", \"E_CO_REGIAO\", \"E_CO_UF\"]\n#regions =  [\"norte\"]\nif __name__ == \"__main__\":\n    if sys.argv[4:]:\n        bucket, year = sys.argv[4:]\n    else:\n        bucket = \"rjr-dados-abertos\"\n        year = \"2020\"\n    escolas = transform(\"escolas\", bucket, year)\n    escolas = add_prefix_in_columns(escolas, \"E\")\n    turmas =  transform(\"turmas\", bucket, year)\n    turmas = add_prefix_in_columns(turmas, \"T\")\n    gestores =  transform(\"gestor\", bucket, year)\n    gestores = add_prefix_in_columns(gestores, \"G\")\n    for region in regions:\n        begin = time()\n        docentes =  transform(\"docentes\", bucket, year, region)\n        docentes = add_prefix_in_columns(docentes, \"D\")\n        matriculas = transform(\"matricula\", bucket, year, region)\n        matriculas = add_prefix_in_columns(matriculas, \"M\")\n        \n        censo = escolas.join(turmas, escolas.E_CO_ENTIDADE == turmas.T_CO_ENTIDADE)\n        censo = censo.join(gestores, censo.E_CO_ENTIDADE == gestores.G_CO_ENTIDADE)\n        censo = censo.join(docentes, censo.T_ID_TURMA == docentes.D_ID_TURMA)\n        censo = censo.join(matriculas, censo.T_ID_TURMA == matriculas.M_ID_TURMA)\n        \n        del(docentes)\n        del(matriculas)\n        censo = censo.drop(*[\"T_CO_ENTIDADE\", \"D_ID_TURMA\", \"M_ID_TURMA\", \"G_CO_ENTIDADE\"])\n        \n        censo \\\n         .write \\\n         .partitionBy(partitions) \\\n         .parquet(f\"gs://{bucket}/processing_zone/censo_escolar\", compression=\"snappy\", mode=\"append\")\n        \n        end = time()\n        \n        print(f\"{region}: {(end-begin)/60} m\")"}, {"cell_type": "code", "execution_count": null, "id": "193ee822-2b41-4b72-a496-b79fa9d70139", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}}, "nbformat": 4, "nbformat_minor": 5}